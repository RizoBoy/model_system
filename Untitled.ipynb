{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850e5123-309f-486e-9a6a-055f0033b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/void/LLVC/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "from model import Net\n",
    "import torch\n",
    "import torchaudio\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from utils import glob_audio_files\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path, config_path):\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    model = Net(**config['model_params'])\n",
    "    model.load_state_dict(torch.load(\n",
    "        checkpoint_path, map_location=\"cpu\")['model'])\n",
    "    return model, config['data']['sr']\n",
    "\n",
    "\n",
    "def load_audio(audio_path, sample_rate):\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    audio = audio.mean(0, keepdim=False)\n",
    "    audio = torchaudio.transforms.Resample(sr, sample_rate)(audio)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def save_audio(audio, audio_path, sample_rate):\n",
    "    torchaudio.save(audio_path, audio, sample_rate)\n",
    "\n",
    "\n",
    "def infer(model, audio):\n",
    "    return model(audio.unsqueeze(0).unsqueeze(0)).squeeze(0)\n",
    "\n",
    "\n",
    "def infer_stream(model, audio, chunk_factor, sr):\n",
    "    L = model.L\n",
    "    chunk_len = model.dec_chunk_size * L * chunk_factor\n",
    "    # pad audio to be a multiple of L * dec_chunk_size\n",
    "    original_len = len(audio)\n",
    "    if len(audio) % chunk_len != 0:\n",
    "        pad_len = chunk_len - (len(audio) % chunk_len)\n",
    "        audio = torch.nn.functional.pad(audio, (0, pad_len))\n",
    "\n",
    "    # scoot audio down by L\n",
    "    audio = torch.cat((audio[L:], torch.zeros(L)))\n",
    "    audio_chunks = torch.split(audio, chunk_len)\n",
    "    # add lookahead context from prev chunk\n",
    "    new_audio_chunks = []\n",
    "    for i, a in enumerate(audio_chunks):\n",
    "        if i == 0:\n",
    "            front_ctx = torch.zeros(L * 2)\n",
    "        else:\n",
    "            front_ctx = audio_chunks[i - 1][-L * 2:]\n",
    "        new_audio_chunks.append(torch.cat([front_ctx, a]))\n",
    "    audio_chunks = new_audio_chunks\n",
    "    print(audio_chunks[0].shape)\n",
    "    outputs = []\n",
    "    times = []\n",
    "    with torch.inference_mode():\n",
    "        enc_buf, dec_buf, out_buf = model.init_buffers(\n",
    "            1, torch.device('cpu'))\n",
    "        if hasattr(model, 'convnet_pre'):\n",
    "            convnet_pre_ctx = model.convnet_pre.init_ctx_buf(\n",
    "                1, torch.device('cpu'))\n",
    "        else:\n",
    "            convnet_pre_ctx = None\n",
    "        for chunk in audio_chunks:\n",
    "            start = time.time()\n",
    "            output, \\\n",
    "                enc_buf, dec_buf, out_buf, \\\n",
    "                convnet_pre_ctx = model(chunk.unsqueeze(\n",
    "                    0).unsqueeze(0),\n",
    "                    enc_buf, dec_buf, out_buf,\n",
    "                    convnet_pre_ctx, pad=(not model.lookahead)\n",
    "                )\n",
    "            outputs.append(output)\n",
    "            times.append(time.time() - start)\n",
    "        # concatenate outputs\n",
    "    outputs = torch.cat(outputs, dim=2)\n",
    "    # Calculate RTF\n",
    "    avg_time = np.mean(times)\n",
    "    rtf = (chunk_len / sr) / avg_time\n",
    "    # calculate e2e latency\n",
    "    e2e_latency = ((2 * L + chunk_len) / sr + avg_time) * 1000\n",
    "    # remove padding\n",
    "    outputs = outputs[:, :, :original_len].squeeze(0)\n",
    "    return outputs, rtf, e2e_latency\n",
    "\n",
    "\n",
    "def do_infer(model, audio, chunk_factor, sr, stream):\n",
    "    with torch.no_grad():\n",
    "        if stream:\n",
    "            outputs, rtf, e2e_latency = infer_stream(\n",
    "                model, audio, chunk_factor, sr)\n",
    "            return outputs, rtf, e2e_latency\n",
    "        else:\n",
    "            outputs = infer(model, audio)\n",
    "            rtf = None\n",
    "            e2e_latency = None\n",
    "    return outputs, rtf, e2e_latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac38aa66-90d4-47ff-8bf0-9b9dacf140ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llvc  llvc_hfg\tllvc_nc\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172d90db-0e66-42dd-8f0a-1a3c21f5ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22797/4280285900.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dir_test ='test_sample'\n",
    "model, sr = load_model(\"checkpoints/llvc/G_500000.pth\", 'experiments/llvc/config.json')\n",
    "os.mkdir('test_sample')\n",
    "# check if fname is a directory\n",
    "audio = load_audio('test_wavs/174-50561-0000.wav', sr)\n",
    "out, rtf, e2e_latency = do_infer(\n",
    "    model, audio, 1, sr, True\n",
    ")\n",
    "# out_fname = os.path.join(\n",
    "    # args.out_dir, os.path.basename(args.fname))\n",
    "save_audio(out, f'{dir_test}/test.wav',sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19d76c47-383b-4026-ae0e-8624412c77b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 240])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "torch.rand((240,)).unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8911536-7f6d-4a2c-932b-1eb562f9eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a978dfa-4f6a-4983-93b8-c8e76b8f9a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.float32}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = set()\n",
    "for k in model.parameters():\n",
    "    test.add(k.dtype)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b40162b-41fc-4603-9be2-82140a81761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discriminators import MultiPeriodDiscriminator, discriminator_loss, generator_loss, feature_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae23c7c4-8687-4b40-bb8b-ad55b4ed3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfg_disc import ComboDisc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef86d7c6-4ee8-4cf9-a2c8-db596228c34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.float32}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_d1 = MultiPeriodDiscriminator([2, 3, 5, 7, 11, 17, 23, 37])\n",
    "for k in test_d1.parameters():\n",
    "    test.add(k.dtype)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b713950-6b0a-4d67-a13a-32a456290865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/void/LLVC/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{torch.float32}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_d1 = ComboDisc()\n",
    "for k in test_d1.parameters():\n",
    "    test.add(k.dtype)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870ff2b2-2e7c-4c4c-9d95-8435ce02987e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (convnet_pre): CachedConvNet(\n",
       "    (down_convs): ModuleList(\n",
       "      (0-11): 12 x ResidualBlock(\n",
       "        (filter): Conv1d(1, 1, kernel_size=(3,), stride=(1,))\n",
       "        (gate): Conv1d(1, 1, kernel_size=(3,), stride=(1,))\n",
       "        (dropout): Dropout1d(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (in_conv): Sequential(\n",
       "    (0): Conv1d(1, 512, kernel_size=(48,), stride=(16,), bias=False)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (label_embedding): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (mask_gen): MaskNet(\n",
       "    (encoder): DilatedCausalConvEncoder(\n",
       "      (dcc_layers): Sequential(\n",
       "        (dcc_0): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dcc_1): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(2,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dcc_2): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(4,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dcc_3): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(8,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dcc_4): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(16,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dcc_5): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(32,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dcc_6): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(64,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dcc_7): DepthwiseSeparableConv(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(128,), groups=512)\n",
       "            (1): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (4): LayerNormPermuted((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_e2d_e): Sequential(\n",
       "      (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), groups=256)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (proj_e2d_l): Sequential(\n",
       "      (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), groups=256)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (proj_d2e): Sequential(\n",
       "      (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), groups=256)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (decoder): CausalTransformerDecoder(\n",
       "      (unfold): Unfold(kernel_size=(26, 1), dilation=1, padding=0, stride=13)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (tf_dec_layers): ModuleList(\n",
       "        (0): CausalTransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_conv): Sequential(\n",
       "    (0): ConvTranspose1d(512, 1, kernel_size=(80,), stride=(16,), padding=(64,), bias=False)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c916c5c1-8043-4b0b-a40a-a3ab97ae3ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64320])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe4501a1-fe70-4c4e-a1d6-8df598b23885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "065ab135-bc9a-496d-a4c9-6cabd7fb5aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_g = draw_graph(model,input_size=(1,1,240), expand_nested=True,device='cpu',save_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93273068-f88a-4269-ae83-227ad398d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_g.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4e68440-477d-4757-8c27-db03723ce87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchview.computation_graph.ComputationGraph at 0x753bf0568dc0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b61dd26-66c1-4d7c-969f-759d24797bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llvc  llvc_hfg\tllvc_nc\n"
     ]
    }
   ],
   "source": [
    "!ls experiments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec09d5f4-117c-42ec-909d-26f8431c0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('experiments/llvc/config.json') as f:\n",
    "    data_comm = json.load(f)\n",
    "\n",
    "with open('experiments/llvc_hfg/config.json') as f:\n",
    "    data_hfg = json.load(f)    \n",
    "\n",
    "with open('experiments/llvc_nc/config.json') as f:\n",
    "    data_nc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61925e36-f361-4ac0-8c55-c86d1c39b9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_len': 1,\n",
       " 'L': 16,\n",
       " 'enc_dim': 512,\n",
       " 'num_enc_layers': 8,\n",
       " 'dec_dim': 256,\n",
       " 'num_dec_layers': 1,\n",
       " 'dec_buf_len': 13,\n",
       " 'dec_chunk_size': 13,\n",
       " 'out_buf_len': 4,\n",
       " 'use_pos_enc': True,\n",
       " 'decoder_dropout': 0.1,\n",
       " 'convnet_config': {'convnet_prenet': True,\n",
       "  'out_channels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'kernel_sizes': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "  'dilations': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'dropout': 0.5,\n",
       "  'combine_residuals': None,\n",
       "  'skip_connection': 'add',\n",
       "  'use_residual_blocks': True}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_comm['model_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e09eaa-7cda-4ec2-b2b6-1bdcd4c76708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_len': 1,\n",
       " 'L': 16,\n",
       " 'enc_dim': 512,\n",
       " 'num_enc_layers': 8,\n",
       " 'dec_dim': 256,\n",
       " 'num_dec_layers': 1,\n",
       " 'dec_buf_len': 13,\n",
       " 'dec_chunk_size': 13,\n",
       " 'out_buf_len': 4,\n",
       " 'use_pos_enc': True,\n",
       " 'decoder_dropout': 0.1,\n",
       " 'convnet_config': {'convnet_prenet': True,\n",
       "  'out_channels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'kernel_sizes': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "  'dilations': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'dropout': 0.5,\n",
       "  'combine_residuals': None,\n",
       "  'skip_connection': 'add',\n",
       "  'use_residual_blocks': True}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hfg['model_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f1018f-4a2b-499b-9e98-653cbd030ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hfg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hfg['discriminator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fafa37e-b33e-4f98-b74f-5cfdd758cebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 11, 17, 23, 37]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nc['periods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5745d244-2534-4028-a7a6-41daa766fd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rvc'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_comm['discriminator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59a84f-c321-43cf-b71b-c0a42f939408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
