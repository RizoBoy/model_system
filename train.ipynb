{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b47c44a6-b28f-4425-bb3c-d2a6161de612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import LLVCDataset as Dataset\n",
    "from model import Net\n",
    "from discriminators import MultiPeriodDiscriminator, discriminator_loss, generator_loss, feature_loss\n",
    "import utils\n",
    "import fairseq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09a95702-ac15-4495-a0f4-2c64e6f6982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LLVCDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, **kwargs\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.rand([1, 240]),torch.rand([1, 240]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7d4fed0-b6cc-41a8-a0e0-1d85b4d507e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data=LLVCDataset()\n",
    "# test_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c04c88a-b1aa-498f-b031-ac954557659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def net_g_step(\n",
    "    batch, net_g, device\n",
    "):\n",
    "    og, gt = batch\n",
    "    og = og.to(device=device, non_blocking=True)\n",
    "    gt = gt.to(device=device, non_blocking=True)\n",
    "\n",
    "    output = net_g(og)\n",
    "    return output, gt, og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64718bf3-4d33-4500-9bd3-8c4b94d01f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def training_runner(\n",
    "    config,\n",
    "    training_dir,\n",
    "):\n",
    "    checkpoint_dir = os.path.join(training_dir, \"checkpoints\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "    data_train = LLVCDataset(\n",
    "        **config['data'], dset='train')\n",
    "    data_val = LLVCDataset(\n",
    "        **config['data'], dset='val')\n",
    "    \n",
    "    for ds in [data_train, data_val]:\n",
    "        print(\n",
    "            f\"Loaded dataset containing {len(ds)} elements\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(data_train,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(data_val,\n",
    "                                             batch_size=1)\n",
    "\n",
    "    net_g = Net(**config['model_params'])\n",
    "    net_g = net_g.to(device=device)\n",
    "\n",
    "    if config['discriminator'] == 'hfg':\n",
    "        net_d = ComboDisc()\n",
    "    else:\n",
    "        net_d = MultiPeriodDiscriminator(periods=config['periods'])\n",
    "    net_d = net_d.to(device=device)\n",
    "\n",
    "    optim_g = torch.optim.AdamW(\n",
    "        net_g.parameters(),\n",
    "        config['optim']['lr'],\n",
    "        betas=config['optim']['betas'],\n",
    "        eps=config['optim']['eps'],\n",
    "        weight_decay=config['optim']['weight_decay']\n",
    "    )\n",
    "    optim_d = torch.optim.AdamW(\n",
    "        net_d.parameters(),\n",
    "        config['optim']['lr'],\n",
    "        betas=config['optim']['betas'],\n",
    "        eps=config['optim']['eps'],\n",
    "        weight_decay=config['optim']['weight_decay']\n",
    "    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    lr = config['optim']['lr']\n",
    "    global_step = 0\n",
    "    epoch = 0\n",
    "\n",
    "    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optim_g, gamma=config['lr_sched']['lr_decay']\n",
    "    )\n",
    "    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optim_d, gamma=config['lr_sched']['lr_decay']\n",
    "    )\n",
    "\n",
    "\n",
    "    # load fairseq model\n",
    "    # if config['aux_fairseq']['c'] > 0:\n",
    "    #     cp_path = config['aux_fairseq']['checkpoint_path']\n",
    "    #     fairseq_model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([\n",
    "    #         cp_path])\n",
    "    #     fairseq_model = fairseq_model[0]\n",
    "    #     # move model to GPU\n",
    "    #     fairseq_model.eval().to(device)\n",
    "    # else:\n",
    "    fairseq_model = None\n",
    "\n",
    "    cache = []\n",
    "    loss_mel_avg = utils.RunningAvg()\n",
    "    loss_fairseq_avg = utils.RunningAvg()\n",
    "    for epoch in range(epoch, 10000):\n",
    "        # train_loader.batch_sampler.set_epoch(epoch)\n",
    "\n",
    "        net_g.train()\n",
    "        net_d.train()\n",
    "\n",
    "        use_cache = len(cache) == len(train_loader)\n",
    "        data = cache if use_cache else enumerate(train_loader)\n",
    "\n",
    "        lr = optim_g.param_groups[0][\"lr\"]\n",
    "\n",
    "        # count down steps to next checkpoint\n",
    "        progress_bar = tqdm(range(config['checkpoint_interval']))\n",
    "        progress_bar.update(global_step % config['checkpoint_interval'])\n",
    "\n",
    "        for batch_idx, batch in data:\n",
    "            output, gt, og = net_g_step(\n",
    "                batch, net_g, device)\n",
    "\n",
    "            # take random slices of input and output wavs\n",
    "            if config['segment_size'] < output.shape[-1]:\n",
    "                start_idx = random.randint(\n",
    "                    0, output.shape[-1] - config['segment_size'] - 1)\n",
    "                gt_sliced = gt[:, :, start_idx:start_idx +\n",
    "                               config['segment_size']]\n",
    "                output_sliced = output.detach()[:, :,\n",
    "                                                start_idx:start_idx + config['segment_size']]\n",
    "            else:\n",
    "                gt_sliced = gt\n",
    "                output_sliced = output.detach()\n",
    "\n",
    "            \n",
    "            y_d_hat_r, y_d_hat_g, _, _ = net_d(\n",
    "                output_sliced, gt_sliced)\n",
    "            loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(\n",
    "                y_d_hat_r, y_d_hat_g\n",
    "            )\n",
    "\n",
    "            optim_d.zero_grad()\n",
    "            loss_disc.backward()\n",
    "            if config['grad_clip_threshold'] is not None:\n",
    "                grad_norm_d = torch.nn.utils.clip_grad_norm_(\n",
    "                    net_d.parameters(), config['grad_clip_threshold'])\n",
    "            grad_norm_d = utils.clip_grad_value_(\n",
    "                net_d.parameters(), config['grad_clip_value'])\n",
    "            optim_d.step()\n",
    "\n",
    "            # Generator\n",
    "            y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(gt, output)\n",
    "            if fairseq_model is not None:\n",
    "                loss_fairseq = utils.fairseq_loss(\n",
    "                    output, gt, fairseq_model) * config['aux_fairseq']['c']\n",
    "            else:\n",
    "                loss_fairseq = torch.tensor(0.0)\n",
    "            loss_fairseq_avg.update(loss_fairseq)\n",
    "           \n",
    "            # if config['aux_mel']['c'] > 0:\n",
    "            #     loss_mel = utils.aux_mel_loss(\n",
    "            #         output, gt, config) * config['aux_mel']['c']\n",
    "            # else:\n",
    "            loss_mel = torch.tensor(0.0)\n",
    "            loss_mel_avg.update(loss_mel)\n",
    "            loss_fm = feature_loss(\n",
    "                fmap_r, fmap_g) * config['feature_loss_c']\n",
    "            loss_gen, losses_gen = generator_loss(\n",
    "                y_d_hat_g)\n",
    "            loss_gen = loss_gen * config['disc_loss_c']\n",
    "            loss_gen_all = (loss_gen + loss_fm) + loss_mel + \\\n",
    "                loss_fairseq\n",
    "\n",
    "            optim_g.zero_grad()\n",
    "            loss_gen_all.backward()\n",
    "            if config['grad_clip_threshold'] is not None:\n",
    "                grad_norm_g = torch.nn.utils.clip_grad_norm_(\n",
    "                    net_g.parameters(), config['grad_clip_threshold'])\n",
    "            grad_norm_g = utils.clip_grad_value_(\n",
    "                net_g.parameters(), config['grad_clip_value'])\n",
    "            optim_g.step()\n",
    "\n",
    "            global_step += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            if global_step > 0 and (global_step % config['log_interval'] == 0):\n",
    "                lr = optim_g.param_groups[0][\"lr\"]\n",
    "                # Amor For Tensorboard display\n",
    "                if loss_mel > 50:\n",
    "                    loss_mel = 50\n",
    "\n",
    "                scalar_dict = {\n",
    "                    \"loss/g/total\": loss_gen_all,\n",
    "                    \"loss/d/total\": loss_disc,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"grad_norm_d\": grad_norm_d,\n",
    "                    \"grad_norm_g\": grad_norm_g,\n",
    "                }\n",
    "                scalar_dict.update(\n",
    "                    {\n",
    "                        \"loss/g/fm\": loss_fm,\n",
    "                        \"loss/g/mel\": loss_mel,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if config['aux_mel']['c'] > 0:\n",
    "                    scalar_dict.update({\"train_metrics/mel\": loss_mel_avg()})\n",
    "                    loss_mel_avg.reset()\n",
    "\n",
    "                if fairseq_model is not None:\n",
    "                    scalar_dict.update(\n",
    "                        {\n",
    "                            \"loss/g/fairseq\": loss_fairseq,\n",
    "                        }\n",
    "                    )\n",
    "                    scalar_dict.update(\n",
    "                        {\"train_metrics/fairseq\": loss_fairseq_avg()}\n",
    "                    )\n",
    "                    loss_fairseq_avg.reset()\n",
    "\n",
    "                scalar_dict.update(\n",
    "                    {\"loss/g/{}\".format(i): v for i,\n",
    "                     v in enumerate(losses_gen)}\n",
    "                )\n",
    "                scalar_dict.update(\n",
    "                    {\"loss/d_r/{}\".format(i): v for i,\n",
    "                     v in enumerate(losses_disc_r)}\n",
    "                )\n",
    "                scalar_dict.update(\n",
    "                    {\"loss/d_g/{}\".format(i): v for i,\n",
    "                     v in enumerate(losses_disc_g)}\n",
    "                )\n",
    "                audio_dict = {}\n",
    "                audio_dict.update(\n",
    "                    {f\"train_audio/gt_{i}\": gt[i].data.cpu().numpy()\n",
    "                     for i in range(min(3, gt.shape[0]))}\n",
    "                )\n",
    "                audio_dict.update(\n",
    "                    {f\"train_audio/in_{i}\": og[i].data.cpu().numpy()\n",
    "                     for i in range(min(3, og.shape[0]))}\n",
    "                )\n",
    "                audio_dict.update(\n",
    "                    {f\"train_audio/pred_{i}\": output[i].data.cpu().numpy()\n",
    "                     for i in range(min(3, output.shape[0]))}\n",
    "                )\n",
    "                net_g.eval()\n",
    "\n",
    "                # load audio from benchmark dir\n",
    "                test_wavs = [\n",
    "                    (\n",
    "                        os.path.basename(p),\n",
    "                        utils.load_wav_to_torch(p, config['data']['sr']),\n",
    "                    )\n",
    "                    for p in glob.glob(config['test_dir'] + \"/*.wav\")\n",
    "                ]\n",
    "\n",
    "                logging.info(\"Testing...\")\n",
    "                for test_wav_name, test_wav in tqdm(test_wavs, total=len(test_wavs)):\n",
    "                    test_out = net_g(test_wav.unsqueeze(\n",
    "                        0).unsqueeze(0).to(device))\n",
    "                    audio_dict.update(\n",
    "                        {f\"test_audio/{test_wav_name}\":\n",
    "                            test_out[0].data.cpu().numpy()}\n",
    "                    )\n",
    "\n",
    "                # don't worry about caching val dataset for now\n",
    "                for loader in [val_loader]:\n",
    "\n",
    "                    loader_name = \"val\"\n",
    "                    v_data = enumerate(loader)\n",
    "                    logging.info(f\"Validating on {loader_name} dataset...\")\n",
    "                    v_loss_mel_avg = utils.RunningAvg()\n",
    "                    v_loss_fairseq_avg = utils.RunningAvg()\n",
    "                    v_mcd_avg = utils.RunningAvg()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for v_batch_idx, v_batch in tqdm(v_data, total=len(loader)):\n",
    "                            v_output, v_gt, og = net_g_step(\n",
    "                                v_batch, net_g, device)\n",
    "\n",
    "                        if config['aux_mel']['c'] > 0:\n",
    "                            v_loss_mel = utils.aux_mel_loss()\n",
    "                            v_loss_mel_avg.update(v_loss_mel)\n",
    "                        if fairseq_model is not None:\n",
    "                            v_loss_fairseq = utils.fairseq_loss(\n",
    "                                output, gt, fairseq_model) * config['aux_fairseq']['c']\n",
    "                            v_loss_fairseq_avg.update(v_loss_fairseq)\n",
    "                        v_mcd = utils.mcd(\n",
    "                            v_output, v_gt, config['data']['sr'])\n",
    "                        v_mcd_avg.update(v_mcd)\n",
    "\n",
    "                    if config['aux_mel']['c'] > 0:\n",
    "                        scalar_dict.update(\n",
    "                            {f\"{loader_name}_metrics/mel\": v_loss_mel_avg(),\n",
    "                             f\"{loader_name}_metrics/mcd\": v_mcd_avg()}\n",
    "                        )\n",
    "                        v_loss_mel_avg.reset()\n",
    "                    if fairseq_model is not None:\n",
    "                        scalar_dict.update(\n",
    "                            {f\"{loader_name}_metrics/fairseq\": v_loss_fairseq_avg()}\n",
    "                        )\n",
    "                        v_loss_fairseq_avg.reset()\n",
    "                    v_mcd_avg.reset()\n",
    "                    audio_dict.update(\n",
    "                        {f\"{loader_name}_audio/gt_{i}\": v_gt[i].data.cpu().numpy()\n",
    "                         for i in range(min(3, v_gt.shape[0]))}\n",
    "                    )\n",
    "                    audio_dict.update(\n",
    "                        {f\"{loader_name}_audio/in_{i}\": og[i].data.cpu().numpy()\n",
    "                         for i in range(min(3, og.shape[0]))}\n",
    "                    )\n",
    "                    audio_dict.update(\n",
    "                        {f\"{loader_name}_audio/pred_{i}\": v_output[i].data.cpu().numpy()\n",
    "                         for i in range(min(3, v_output.shape[0]))}\n",
    "                    )\n",
    "\n",
    "                net_g.train()\n",
    "\n",
    "                utils.summarize(\n",
    "                    writer=writer,\n",
    "                    global_step=global_step,\n",
    "                    scalars=scalar_dict,\n",
    "                    audios=audio_dict,\n",
    "                    audio_sampling_rate=config['data']['sr'],\n",
    "                )\n",
    "\n",
    "                if global_step % config['checkpoint_interval'] == 0:\n",
    "                    g_checkpoint = os.path.join(\n",
    "                        checkpoint_dir, f\"G_{global_step}.pth\")\n",
    "                    d_checkpoint = os.path.join(\n",
    "                        checkpoint_dir, f\"D_{global_step}.pth\")\n",
    "                    utils.save_state(\n",
    "                        net_g,\n",
    "                        optim_g,\n",
    "                        lr,\n",
    "                        epoch,\n",
    "                        global_step,\n",
    "                        g_checkpoint\n",
    "                    )\n",
    "                    utils.save_state(\n",
    "                        net_d,\n",
    "                        optim_d,\n",
    "                        lr,\n",
    "                        epoch,\n",
    "                        global_step,\n",
    "                        d_checkpoint\n",
    "                    )\n",
    "                    logging.info(\n",
    "                        f\"Saved checkpoints to {g_checkpoint} and {d_checkpoint}\")\n",
    "                    progress_bar.reset()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        scheduler_g.step()\n",
    "        scheduler_d.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "addfe091-5d4d-4f81-a096-e918d5f0445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('experiments/llvc/config.json') as f:\n",
    "    config = json.load(f)\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed7bb9c2-efc7-432d-a32b-a7ceb68b6a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset containing 100 elements\n",
      "Loaded dataset containing 100 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                                     | 0/5000 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 1/5000 [00:02<3:04:03,  2.21s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 2/5000 [00:03<2:40:51,  1.93s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 3/5000 [00:05<2:31:23,  1.82s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 4/5000 [00:07<2:26:34,  1.76s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 5/5000 [00:08<2:23:26,  1.72s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 6/5000 [00:10<2:21:34,  1.70s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 7/5000 [00:12<2:20:24,  1.69s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 8/5000 [00:13<2:19:51,  1.68s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                           | 9/5000 [00:15<2:20:14,  1.69s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                          | 10/5000 [00:17<2:19:55,  1.68s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                          | 11/5000 [00:19<2:21:11,  1.70s/it]\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                          | 12/5000 [00:20<2:21:42,  1.70s/it]\n",
      "\n",
      "  0%|                                                                                                                                                                                      | 0/5000 [01:40<?, ?it/s]\n",
      "  0%|                                                                                                                                                                                      | 0/5000 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 119\u001b[0m, in \u001b[0;36mtraining_runner\u001b[0;34m(config, training_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m loss_disc, losses_disc_r, losses_disc_g \u001b[38;5;241m=\u001b[39m discriminator_loss(\n\u001b[1;32m    115\u001b[0m     y_d_hat_r, y_d_hat_g\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m optim_d\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 119\u001b[0m \u001b[43mloss_disc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_clip_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     grad_norm_d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m    122\u001b[0m         net_d\u001b[38;5;241m.\u001b[39mparameters(), config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_clip_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/LLVC/venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLVC/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLVC/venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_runner(config, 'test_train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15cfc09-2f75-4fa8-ae6b-014daf86d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from scipy.io.wavfile import read\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_dataset(dir):\n",
    "    original_files = glob.glob(os.path.join(dir, \"*_original.wav\"))\n",
    "    converted_files = []\n",
    "    for original_file in original_files:\n",
    "        converted_file = original_file.replace(\n",
    "            \"_original.wav\", \"_converted.wav\")\n",
    "        converted_files.append(converted_file)\n",
    "    return original_files, converted_files\n",
    "\n",
    "\n",
    "def load_wav(full_path):\n",
    "    sampling_rate, data = read(full_path)\n",
    "    return data, sampling_rate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80fb7044-b527-4d8b-b9d9-a9a1d6fb66d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174-50561-0000.wav    2902-9006-0000.wav   7850-73752-0000.wav\n",
      "1919-142785-0000.wav  5895-34615-0000.wav  8842-302196-0000.wav\n",
      "2086-149214-0000.wav  652-129742-0000.wav\n",
      "2412-153947-0000.wav  777-126732-0000.wav\n"
     ]
    }
   ],
   "source": [
    "!ls test_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f4211c-c322-4f00-bd8e-99f1344a9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data, o_sr = load_wav(\"test_wavs/174-50561-0000.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666a1a72-2b29-4bc6-a22c-3fc52b6acdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int16')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3567de2d-e642-440e-a278-aa248ec2a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted = torch.from_numpy(original_data)\n",
    "converted = converted.unsqueeze(0).to(torch.float) / 32768\n",
    "if converted.shape[-1] < 240:\n",
    "    converted = torch.cat(\n",
    "        (converted, torch.zeros(1, 240 - converted.shape[-1])), dim=1\n",
    "    )\n",
    "else:\n",
    "    converted = converted[:, : 240]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5739b72f-88b9-4a18-9001-1b599303145c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd4cf36-2216-4b81-a512-4f753de2bb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce49be-169f-4071-a31b-f92c354dfeda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
